<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>决策树--1.决策树的构造 - 奔赴下一场山海</title><meta name=keywords content="博客,程序员,架构师,思考,读书,笔记,技术,分享,大数据,产品"><meta name=author content="Murphy"><meta property="og:title" content="决策树--1.决策树的构造"><meta property="og:site_name" content="奔赴下一场山海"><meta property="og:image" content="/img/author.jpg"><meta name=title content="决策树--1.决策树的构造 - 奔赴下一场山海"><meta name=description content="欢迎来到Murphy的博客网站。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i>
<a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span>
<span class=site-title>奔赴下一场山海</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>但远山长，云山乱，晓山青。</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span>
<span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://murphyhanxu.github.io/post/decisiontree1/ itemprop=url>决策树--1.决策树的构造</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i>
<span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2022-08-04">2022-08-04</time></span>
<span class=post-category>&nbsp; | &nbsp;
<i class="fa fa-folder-o fa-fw"></i>
<span class=post-meta-item-text>分类：</span>
<span itemprop=about itemscope itemtype=https://schema.org/Thing><a href=/categories/cs itemprop=url rel=index style=text-decoration:underline><span itemprop=name>CS</span></a>
&nbsp;</span></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i>
<span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>6171 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i>
<span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>13分钟</span></span>
<span id=/post/decisiontree1/ class=leancloud_visitors data-flag-title=决策树--1.决策树的构造>|
<i class="fa fa-binoculars fa-fw"></i>
<span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><p>本章构造的决策树算法能够读取数据集合，构建类似于图1的决策树。</p><p>决策树很多任务都是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，机器学习算法最终将使用这些机器从数据集中创造的规则。</p><p>专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。</p><p><img src=https://murphyhanxu.github.io/blogs-images/images/DecisionTree1.1.png alt=DecisionTree1.1></p><h2 id=1--决策树的构造>1. 决策树的构造</h2><p>决策树</p><p>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。</p><p>缺点：可能会产生过度匹配问题。</p><p>适用数据类型：数值型和标称型。</p><p>我们将一步步地构造决策树算法，并会涉及许多有趣的细节。首先我们讨论数学上如何使用信息论划分数据集，然后编写代码将理论应用到具体的数据集上，最后编写代码构造决策树。</p><p>在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前无需阅读的垃圾邮件已经正确地划分数据分类，无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。如何划分数据子集的算法和划分原始数据集的方法相同，知道所有具有相同类型的数据均在一个数据子集内。</p><p>创建分支的伪代码函数createBranch()如下所示：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>检测数据集中的每个子项是否属于同一分类：
</span></span><span style=display:flex><span>	If so return 类标签:
</span></span><span style=display:flex><span>	Else
</span></span><span style=display:flex><span>		寻找划分数据集的最好特征
</span></span><span style=display:flex><span>		划分数据集
</span></span><span style=display:flex><span>		创建分支节点
</span></span><span style=display:flex><span>			for 每个划分的子集
</span></span><span style=display:flex><span>				调用函数createBranch并增加返回结果到分支节点中
</span></span><span style=display:flex><span>		return 分支节点
</span></span></code></pre></div><p>上面的伪代码createBranch是一个递归函数，在倒数第二行直接调用了它自己。后面将把上述的伪代码转换为Python代码，这里我们需要进一步了解算法是如何划分数据集的。</p><p>决策树的一般流程</p><p>1.收集数据：可以使用任何方法</p><p>2.准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。</p><p>3.分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。</p><p>4.训练数据：构造树的数据结构。</p><p>5.测试算法：使用经验树计算错误率。</p><p>6.使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。</p><p>下表包含5个海洋动物，特征包括：不浮出水面是否可以生存，以及是否有脚蹼。我们可以将这些动物分成两类：鱼类和非鱼类。现在我们想要决定依据第一个特征还是第二个特征划分数据。在回答这个问题之前，我们必须采用量化的方法判断如何划分数据。</p><table><thead><tr><th>编号</th><th>不浮出水面是否可以生存</th><th>是否有脚蹼</th><th>属于鱼类</th></tr></thead><tbody><tr><td>1</td><td>是</td><td>是</td><td>是</td></tr><tr><td>2</td><td>是</td><td>是</td><td>是</td></tr><tr><td>3</td><td>是</td><td>否</td><td>否</td></tr><tr><td>4</td><td>否</td><td>是</td><td>否</td></tr><tr><td>5</td><td>否</td><td>是</td><td>否</td></tr></tbody></table><h3 id=11--信息增益>1.1 信息增益</h3><p>划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学。</p><p>我们可以在划分数据之前使用信息论量化度量信息的内容。</p><p>在划分数据集之前之后信息发生的变化称为信息增益（information gain），知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。</p><p>在可以评测哪种数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵（entropy），这个名字来源于信息论之父克劳德香农。</p><p>熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事务可能划分在多个分类之中，则符号$ x_i $的信息定义为</p><p>$$
l(x_i)=-\log_2 p(x_i)
$$
其中$p(x_i)$是选择该分类的概率。</p><p>为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：
$$
H=-\sum_{i=1}^np(x_i)\log_2p(x_i)
$$
其中$n$是分类的数目。</p><p>下面我们将使用Python计算信息熵，创建名为trees.py的文件，此代码的功能是计算给定数据集的熵。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#080;font-style:italic>#  程序清单1.1.1 计算给定数据集的香农熵</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>math</span> <span style=color:#a2f;font-weight:700>import</span> log
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>calcShannonEnt</span>(dataSet):
</span></span><span style=display:flex><span>    numEntries <span style=color:#666>=</span> <span style=color:#a2f>len</span>(dataSet)
</span></span><span style=display:flex><span>    labelCounts <span style=color:#666>=</span> {}
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic>#  ①为所有可能分类创建字典</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> featVec <span style=color:#a2f;font-weight:700>in</span> dataSet:
</span></span><span style=display:flex><span>        currentLabel <span style=color:#666>=</span> featVec[<span style=color:#666>-</span><span style=color:#666>1</span>]
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>if</span> currentLabel <span style=color:#a2f;font-weight:700>not</span> <span style=color:#a2f;font-weight:700>in</span> labelCounts<span style=color:#666>.</span>keys():
</span></span><span style=display:flex><span>            labelCounts[currentLabel] <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>        labelCounts[currentLabel] <span style=color:#666>+=</span> <span style=color:#666>1</span>
</span></span><span style=display:flex><span>    shannonEnt <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic>#  ②以2为底求对数</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> key <span style=color:#a2f;font-weight:700>in</span> labelCounts:
</span></span><span style=display:flex><span>        prob <span style=color:#666>=</span> <span style=color:#a2f>float</span>(labelCounts[key])<span style=color:#666>/</span>numEntries
</span></span><span style=display:flex><span>        shannonEnt <span style=color:#666>-=</span> prob <span style=color:#666>*</span> log(prob, <span style=color:#666>2</span>)
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> shannonEnt
</span></span></code></pre></div><p>首先，计算数据集中实例的总数。我们也可以在需要时再计算这个值，但是由于代码中多次用到这个值，为了提高代码效率，我们显式地声明一个变量保存实例总数。然后，创建一个数据字典，它的键值是最后一列的数值①。如果当前键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。最后，使用所有类标签的发生频率计算类别分别出现的概率。我们将用这个概率计算香农熵②，统计所有类标签发生的次数。</p><p>下面我们看看如何使用熵划分数据集。</p><p>在trees.py中，我们利用creaDataSet()函数得到表的简单数据集。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>createDataSet</span>():
</span></span><span style=display:flex><span>    dataSet <span style=color:#666>=</span> [[<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#34;yes&#34;</span>],
</span></span><span style=display:flex><span>               [<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#34;yes&#34;</span>],
</span></span><span style=display:flex><span>               [<span style=color:#666>1</span>, <span style=color:#666>0</span>, <span style=color:#b44>&#34;no&#34;</span>],
</span></span><span style=display:flex><span>               [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#34;no&#34;</span>],
</span></span><span style=display:flex><span>               [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#34;no&#34;</span>]]
</span></span><span style=display:flex><span>    labels <span style=color:#666>=</span> [<span style=color:#b44>&#34;no surfacing&#34;</span>, <span style=color:#b44>&#34;flippers&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> dataSet, labels
</span></span></code></pre></div><p>在Python命令提示符下输入下列命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>reload(trees<span style=color:#666>.</span>py)
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat, label <span style=color:#666>=</span> trees<span style=color:#666>.</span>createDataSet()
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat
</span></span><span style=display:flex><span>[[<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;maybe&#39;</span>], [<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;yes&#39;</span>], [<span style=color:#666>1</span>, <span style=color:#666>0</span>, <span style=color:#b44>&#39;no&#39;</span>], [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;no&#39;</span>], [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;no&#39;</span>]]
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>trees<span style=color:#666>.</span>calcShannonEnt(myDat)
</span></span><span style=display:flex><span><span style=color:#666>0.9287712379549449</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>#  熵越高，则混合的数据也越多，我们可以在数据集中添加更多的分类，观察熵是如何变化的。这里我们增加第三个名为maybe的分类，测试熵的变化：</span>
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat[<span style=color:#666>0</span>][<span style=color:#666>-</span><span style=color:#666>1</span>] <span style=color:#666>=</span> <span style=color:#b44>&#34;maybe&#34;</span>
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat
</span></span><span style=display:flex><span>[[<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;maybe&#39;</span>], [<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;yes&#39;</span>], [<span style=color:#666>1</span>, <span style=color:#666>0</span>, <span style=color:#b44>&#39;no&#39;</span>], [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;no&#39;</span>], [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;no&#39;</span>]]
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>trees<span style=color:#666>.</span>calcShannonEnt(myDat)
</span></span><span style=display:flex><span><span style=color:#666>1.3931568569324173</span>
</span></span></code></pre></div><p>得到熵之后，我们就可以按照获取最大信息增益的方法划分数据集，下一节我们讨论如何划分数据集并创建决策树，以及如何度量信息增益。</p><h3 id=12--划分数据集>1.2 划分数据集</h3><p>上节我们学习了如何度量数据集里的无序程度，分类算法除了需要测量信息熵，还需要划分数据集，度量花费数据集的熵，以便判断当前是否正确地划分了数据集。我们将对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。想象一下分布在二维空间的数据散点图，需要在数据之间划条线，将它们分成两部分，我们接下来将讨论是按照$x$$轴还是$$y$轴划线呢？</p><p>要划分数据集，在trees.py文件下输入下列的代码：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#080;font-style:italic>#  程序清单1.2.1 按照给定特征划分数据集</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>splitDataSet</span>(dataSet, axis, value):
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic>#  ①创建新的list对象</span>
</span></span><span style=display:flex><span>    retDataSet <span style=color:#666>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> featVec <span style=color:#a2f;font-weight:700>in</span> dataSet:
</span></span><span style=display:flex><span>        <span style=color:#080;font-style:italic>#  ②抽取</span>
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>if</span> featVec[axis] <span style=color:#666>==</span> value:
</span></span><span style=display:flex><span>            reducedFeatVec <span style=color:#666>=</span> featVec[:axis]
</span></span><span style=display:flex><span>            reducedFeatVec<span style=color:#666>.</span>extend(featVec[axis<span style=color:#666>+</span><span style=color:#666>1</span>:])
</span></span><span style=display:flex><span>            retDataSet<span style=color:#666>.</span>append(reducedFeatVec)
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> retDataSet
</span></span></code></pre></div><p>程序清单1.2.1使用了三个输入参数：待划分的数据集、划分数据集的特征、特征的返回值。</p><p>需要注意的是，Python语言不用考虑内存分配问题。Python语言在函数中传递的是列表的引用，在函数内部对列表对象的修改，将会影响该列表对象的整个生存周期。为了消除这个不良影响，我们需要在函数的开始声明一个新列表对象。因为该函数代码在同一数据集上被调用多次，为了不修改原始数据集，创建一个新的列表对象①。数据集这个列表中的各个元素列表，我们要遍历数据集中的每个元素，一旦发现符合要求的值，则将其添加到新创建的列表中。</p><p>在if语句中，程序将符合特征的数据抽取出来②。后面讲述得更简单，这里我们可以这样理解这段代码：当我们按照某个特征划分数据集时，就需要将所有符合要求的元素抽取出来。</p><p>代码中使用了Python语言列表类型自带的extend()和append()方法。这两个方法功能类似，但是在处理多个列表时，这两个方法的处理结果是完全不同的。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#080;font-style:italic>#  假定存在两个列表，a和b：</span>
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>a <span style=color:#666>=</span> [<span style=color:#666>1</span>, <span style=color:#666>2</span>, <span style=color:#666>3</span>]
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>b <span style=color:#666>=</span> [<span style=color:#666>4</span>, <span style=color:#666>5</span>, <span style=color:#666>6</span>]
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>a<span style=color:#666>.</span>append(b)
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>a
</span></span><span style=display:flex><span>[<span style=color:#666>1</span>, <span style=color:#666>2</span>, <span style=color:#666>3</span>, [<span style=color:#666>4</span>, <span style=color:#666>5</span>, <span style=color:#666>6</span>]]
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>#  如果执行a.append(b)，则列表得到了第四个元素，而且第四个元素也是一个列表。然而如果使用.extend()方法：</span>
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>a <span style=color:#666>=</span> [<span style=color:#666>1</span>, <span style=color:#666>2</span>, <span style=color:#666>3</span>]
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>a<span style=color:#666>.</span>extend(b)
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>a
</span></span><span style=display:flex><span>[<span style=color:#666>1</span>, <span style=color:#666>2</span>, <span style=color:#666>3</span>, <span style=color:#666>4</span>, <span style=color:#666>5</span>, <span style=color:#666>6</span>]
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>#  则得到一个包含a和b所有元素的列表。</span>
</span></span></code></pre></div><p>我们可以在前面的简单样本数据上测试函数splitDataSet()。首先还是要将程序清单1.2.1的代码增加到trees.py文件中，然后再Python命令提示符内输入下述命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>reload(trees)
</span></span><span style=display:flex><span><span style=color:#666>&lt;</span>module <span style=color:#b44>&#39;trees&#39;</span> <span style=color:#a2f;font-weight:700>from</span> <span style=color:#b44>&#39;trees.pyc&#39;</span><span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat, labels<span style=color:#666>=</span>trees<span style=color:#666>.</span>createDataSet()
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat
</span></span><span style=display:flex><span>[[<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#34;yes&#34;</span>], [<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#34;yes&#34;</span>], [<span style=color:#666>1</span>, <span style=color:#666>0</span>, <span style=color:#b44>&#34;no&#34;</span>], [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#34;no&#34;</span>], [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#34;no&#34;</span>]]
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>trees<span style=color:#666>.</span>splitDataSet(myDat, <span style=color:#666>0</span>, <span style=color:#666>1</span>)
</span></span><span style=display:flex><span>[[<span style=color:#666>1</span>, <span style=color:#b44>&#34;yes&#34;</span>], [<span style=color:#666>1</span>, <span style=color:#b44>&#34;yes&#34;</span>], [<span style=color:#666>0</span>, <span style=color:#b44>&#34;no&#34;</span>]]
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>trees<span style=color:#666>.</span>splitDataSet(myDat, <span style=color:#666>0</span>, <span style=color:#666>0</span>)
</span></span><span style=display:flex><span>[[<span style=color:#666>1</span>, <span style=color:#b44>&#34;no&#34;</span>], [<span style=color:#666>1</span>, <span style=color:#b44>&#34;no&#34;</span>]]
</span></span></code></pre></div><p>接下来我们将遍历整个数据集，循环计算香农熵和splitDataSet()函数，找到最好的特征划分方式。熵计算将会告诉我们如何划分数据集时最好的数据组织方式。</p><p>在trees.py文件中输入下面的程序代码。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#080;font-style:italic>#  程序清单1.2.2 选择最好的数据集划分方式</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>chooseBestFeatureToSplit</span>(dataSet):
</span></span><span style=display:flex><span>    numFeatures <span style=color:#666>=</span> <span style=color:#a2f>len</span>(dataSet[<span style=color:#666>0</span>]) <span style=color:#666>-</span> <span style=color:#666>1</span>
</span></span><span style=display:flex><span>    baseEntropy <span style=color:#666>=</span> calcShannonEnt(dataSet)
</span></span><span style=display:flex><span>    bestInfoGain <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>    bestFeature <span style=color:#666>=</span> <span style=color:#666>-</span><span style=color:#666>1</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> i <span style=color:#a2f;font-weight:700>in</span> <span style=color:#a2f>range</span>(numFeatures):
</span></span><span style=display:flex><span>        <span style=color:#080;font-style:italic>#  ①创建唯一的分类标签列表</span>
</span></span><span style=display:flex><span>        featList <span style=color:#666>=</span> [example[i] <span style=color:#a2f;font-weight:700>for</span> example <span style=color:#a2f;font-weight:700>in</span> dataSet]
</span></span><span style=display:flex><span>        uniqueVals <span style=color:#666>=</span> <span style=color:#a2f>set</span>(featList)
</span></span><span style=display:flex><span>        newEntropy <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>for</span> value <span style=color:#a2f;font-weight:700>in</span> uniqueVals:
</span></span><span style=display:flex><span>            <span style=color:#080;font-style:italic>#  ②计算每种划分方式的信息熵</span>
</span></span><span style=display:flex><span>            subDataSet <span style=color:#666>=</span> splitDataSet(dataSet, i, value)
</span></span><span style=display:flex><span>            prob <span style=color:#666>=</span> <span style=color:#a2f>len</span>(subDataSet)<span style=color:#666>/</span><span style=color:#a2f>float</span>(<span style=color:#a2f>len</span>(dataSet))
</span></span><span style=display:flex><span>            newEntroy <span style=color:#666>+=</span> prob <span style=color:#666>*</span> calcShannonEnt(subDataSet)
</span></span><span style=display:flex><span>        infoGain <span style=color:#666>=</span> baseEntropy <span style=color:#666>-</span> newRntropy
</span></span><span style=display:flex><span>        <span style=color:#080;font-style:italic>#  ③计算最好的信息增益</span>
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>if</span> (infoGain <span style=color:#666>&gt;</span> bestInfoGain):
</span></span><span style=display:flex><span>            bestInfoGain <span style=color:#666>=</span> infoGain
</span></span><span style=display:flex><span>            bestFeature <span style=color:#666>=</span> i
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> bestFeature
</span></span></code></pre></div><p>程序清单1.2.2给出了函数chooseBestFeatureToSplit()的完整代码，该函数实现选取特征，划分数据集，计算得出最好的划分数据集的特征。函数chooseBestFeatureToSplit()使用了程序清单1.1.1和1.2.1中的函数。在函数调用的数据需要满足一定的要求：第一个要求是，数据必须是一种由列表元素组成的列表，而且所有的列表元素都要具有相同的数据长度；第二个要求是，数据的最后一列或者每个实例的最后一个元素是当前实例的类别标签。数据集一旦满足上述要求，我们就可以在函数的第一行判定当前数据集包含多少特征属性。我们无需限定list中的数据类型，它们既可以是数字也可以是字符串，并不影响实际计算。</p><p>在开始划分数据集之前，程序清单1.2.2的第3行代码计算了整个数据集的原始香农熵，我们保存最初的无序度量值，用于与划分完之后的数据集计算的熵值进行比较。第1个for循环遍历数据集中的所有特征。使用列表推导来创建新的列表，将数据集中所有第i个特征值或者所有可能存在的值写入这个新list中①。然后使用Python的集合（set）数据类型。集合数据类型与列表类型相似，不同之处仅在于集合类型中的每个值互不相同。从列表中创建集合是Python语言得到列表中唯一元素值的最快方法。</p><p>遍历当前特征中的所有唯一属性值，对每个特征划分一次数据集②，然后计算数据集的新熵值，并对所有唯一特征值得到的熵求和。信息增益是熵减或是数据无序度的减少，大家肯定对于将熵用于度量数据无序度的减少更容易理解。最后，比较所有特征中的信息增益，返回最好特征划分的索引值③。</p><p>现在我们可以测试上面代码的实际输出结果，首先将程序清单1.2.2的内容输入到文件trees.py中，然后在Python命令提示符下输入下列命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>reload(trees)
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat, labels <span style=color:#666>=</span> trees<span style=color:#666>.</span>createDataSet()
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>trees<span style=color:#666>.</span>chooseBestFeatureToSplit(myDat)
</span></span><span style=display:flex><span><span style=color:#666>0</span>
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat
</span></span><span style=display:flex><span>[[<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;yes&#39;</span>], [<span style=color:#666>1</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;yes&#39;</span>], [<span style=color:#666>1</span>, <span style=color:#666>0</span>, <span style=color:#b44>&#39;no&#39;</span>], [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;no&#39;</span>], [<span style=color:#666>0</span>, <span style=color:#666>1</span>, <span style=color:#b44>&#39;no&#39;</span>]]
</span></span></code></pre></div><p>代码运行结果告诉我们，第0个特征是最好的用于划分数据集的特征。让我们来观察一下变量myDat中的数据。按照上述的方法划分数据集，第一个特征为1的海洋生物分组将有两个属于鱼类，一个属于非鱼类；另一个分组则全部属于非鱼类。如果按照第二个特征分组，结果又是怎样呢？第一个海洋动物分组将有两个属于鱼类，两个属于非鱼类；另一个分组则只有一个非鱼类。因此，第0个特征确实是最好的用于划分数据集的特征。</p><p>下一节我们将介绍如何将这些函数功能放在一起，构建决策树。</p><h3 id=13--递归构建决策树>1.3 递归构建决策树</h3><p>目前我们已经学习了从数据集构造决策树算法所需要的子功能模块，其工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。</p><p>递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块。任何到达叶子节点的数据必然属于叶子节点的分类，如下图所示。</p><p><img src=https://murphyhanxu.github.io/blogs-images/images/DecisionTree1.2.png alt=DecisionTree2></p><p>第一个结束条件使得算法可以终止，我们甚至可以设置算法可以划分的最大分组数目。后续章节还会介绍其他决策树算法，如C4.5和CART，这些算法在运行时并不总是在每次划分分组时都会消耗特征。由于特征数目并不是在每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>operator</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>majorityCnt</span>(classList):
</span></span><span style=display:flex><span>    classCount<span style=color:#666>=</span>{}
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> vote <span style=color:#a2f;font-weight:700>in</span> classList:
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>if</span> vote <span style=color:#a2f;font-weight:700>not</span> <span style=color:#a2f;font-weight:700>in</span> classCount<span style=color:#666>.</span>keys(): classCount[vote] <span style=color:#666>=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>        classCount[vote] <span style=color:#666>+=</span> <span style=color:#666>1</span>
</span></span><span style=display:flex><span>    sortedClassCount <span style=color:#666>=</span> <span style=color:#a2f>sorted</span>(classCount<span style=color:#666>.</span>iteritems(), key<span style=color:#666>=</span>operator<span style=color:#666>.</span>itemgetter(<span style=color:#666>1</span>), reverse<span style=color:#666>=</span><span style=color:#a2f;font-weight:700>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> sortedClassCount[<span style=color:#666>0</span>][<span style=color:#666>0</span>]
</span></span></code></pre></div><p>上面的代码与1.2.2的投票表决代码非常类似，该函数使用分类名称的列表，然后创建键值为classList中唯一值的数据字典，字典对象存储量classList中每个类标签出现的频率，最后利用operator操作键值排序字典，并返回出现次数最多的分类名称。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#080;font-style:italic>#  程序清单1.3.1 创建树的函数代码</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>createTree</span>(dataSet,labels):
</span></span><span style=display:flex><span>    classList <span style=color:#666>=</span> [example[<span style=color:#666>-</span><span style=color:#666>1</span>] <span style=color:#a2f;font-weight:700>for</span> example <span style=color:#a2f;font-weight:700>in</span> dataSet]
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic>#  ①类别完全相同则停止继续划分</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>if</span> classList<span style=color:#666>.</span>count(classList[<span style=color:#666>0</span>]) <span style=color:#666>==</span> <span style=color:#a2f>len</span>(classList): 
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>return</span> classList[<span style=color:#666>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>if</span> <span style=color:#a2f>len</span>(dataSet[<span style=color:#666>0</span>]) <span style=color:#666>==</span> <span style=color:#666>1</span>: 
</span></span><span style=display:flex><span>        <span style=color:#080;font-style:italic>#  ②遍历完所有特征时返回出现次数最多的</span>
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>return</span> majorityCnt(classList)
</span></span><span style=display:flex><span>    bestFeat <span style=color:#666>=</span> chooseBestFeatureToSplit(dataSet)
</span></span><span style=display:flex><span>    bestFeatLabel <span style=color:#666>=</span> labels[bestFeat]
</span></span><span style=display:flex><span>    myTree <span style=color:#666>=</span> {bestFeatLabel:{}}
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>del</span>(labels[bestFeat])
</span></span><span style=display:flex><span>    featValues <span style=color:#666>=</span> [example[bestFeat] <span style=color:#a2f;font-weight:700>for</span> example <span style=color:#a2f;font-weight:700>in</span> dataSet]
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic>#  ③得到列表包含的所有属性值</span>
</span></span><span style=display:flex><span>    uniqueVals <span style=color:#666>=</span> <span style=color:#a2f>set</span>(featValues)
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> value <span style=color:#a2f;font-weight:700>in</span> uniqueVals:
</span></span><span style=display:flex><span>        subLabels <span style=color:#666>=</span> labels[:]       <span style=color:#080;font-style:italic>#copy all of labels, so trees don&#39;t mess up existing labels</span>
</span></span><span style=display:flex><span>        myTree[bestFeatLabel][value] <span style=color:#666>=</span> createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> myTree     
</span></span></code></pre></div><p>程序清单1.3.1的代码使用两个输入参数：数据集和标签列表。标签列表包含了数据集中所有特征的标签，算法本身并不需要这个变量，但是为了给出数据明确的含义，我们将它作为一个输入参数提供。此外，前面提到的对数据集的要求这里依然需要满足。上述代码首先创建了名为classList的列表变量，其中包含了数据集的所有类标签。递归函数的第一个停止条件是所有的类标签完全相同，则直接返回该类标签①。</p><p>递归函数的第二个停止条件是使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组②。由于第二个条件无法简单地返回唯一的类标签，这里使用程序清单1.3.1的函数挑选出现次数最多的类别作为返回值。</p><p>下一步程序开始创建树，这里使用Python语言的字典类型存储树的信息，当然也可以声明特殊的数据类型存储树，但这里完全没有必要。字典变量myTree存储了树的所有信息，这对于其后绘制树形图非常重要。当前数据集选取的最好特征存储在变量bestFeat中，得到列表包含的所有属性值③。这部分代码与程序清单1.3.1的部分代码类似。</p><p>最后代码遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree()，得到的返回值将被插入到字典变量myTree中，因此函数终止执行时，字典中将会嵌套很多代表叶子节点信息的字典数据。在解释这个嵌套数据之前，我们先看一下循环的第一行subLabels = labels[:]，这行代码复制了类标签，参数是按照引用方式传递的。为了保证每次调用函数createTree()时不改变原始列表的内容，使用新变量subLabels代替原始列表。</p><p>在Python命令提示符下输入下列命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>reload(trees)
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myDat, labels <span style=color:#666>=</span> trees<span style=color:#666>.</span>createDataSet()
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myTree <span style=color:#666>=</span> trees<span style=color:#666>.</span>createTree(myDat, labels)
</span></span><span style=display:flex><span><span style=color:#666>&gt;&gt;&gt;</span>myTree
</span></span><span style=display:flex><span>{<span style=color:#b44>&#34;no surfacing&#34;</span>:{<span style=color:#666>0</span>:<span style=color:#b44>&#34;no&#34;</span>,<span style=color:#666>1</span>:{<span style=color:#b44>&#34;flippers&#34;</span>:{<span style=color:#666>0</span>:<span style=color:#b44>&#34;no&#34;</span>, <span style=color:#666>1</span>:<span style=color:#b44>&#34;yes&#34;</span>}}}}
</span></span></code></pre></div><p>变量myTree包含了很多代表树结构信息的嵌套字典，从左边开始，第一个关键字nosurfacing是第一个划分数据集的特征名称，该关键字的值也是另一个数据字典。第二个关键字是no surfacing特征划分的数据集，这些关键字的值是no surfacing节点的子节点。这些值可能是类标签，也可能是另一个数据字典。如果值是类标签，则该子节点是叶子节点；如果值是另一个数据字典，则子节点是一个判断节点，这种格式结构不断重复就肿成了整棵树。本节的例子中，这棵树包含了3个叶子节点以及2个判断节点。</p><p>下一章将介绍如何绘制图形，方便我们正确理解数据信息的内在含义。</p></div><footer class=post-footer><div class=post-tags><a href=/tags/machine-learning rel=tag title="Machine Learning">#Machine Learning#</a></div><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.jpg width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>决策树--1.决策树的构造</p><p><span>链接：</span>https://murphyhanxu.github.io/post/decisiontree1/</p><p><span>作者：</span>Murphy</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick='var qr=document.getElementById("QR");qr.style.display==="none"?qr.style.display="block":qr.style.display="none"'>
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://murphyhanxu.github.io/post/decisiontree2/ rel=next title=决策树--2.绘制树形图><i class="fa fa-chevron-left"></i> 决策树--2.绘制树形图</a></div><div class="post-nav-prev post-nav-item"><a href=https://murphyhanxu.github.io/post/autoexcel/ rel=prev title=Python(xlrd,xlwt)对于excel的操作>Python(xlrd,xlwt)对于excel的操作
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
<span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
<span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>文章目录</li><li class=sidebar-nav-overview data-target=site-overview>站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/avatar.png alt=Murphy><p class=site-author-name itemprop=name>Murphy</p><p class="site-description motion-element" itemprop=description>谁的脸谁的姓名</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>18</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>1</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>8</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/MurphyHanxu target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>
GitHub</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li><li class=links-of-blogroll-item><a href=https://math.mickeylili.com/ title=Mickeylili target=_blank>Mickeylili</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>
标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/machine-learning>Machine learning</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/github-pages>Github pages</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python>Python</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/hugo>Hugo</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/sql>SQL</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/didl>Didl</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/math>Math</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/mathematical-modeling>Mathematical modeling</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1--决策树的构造>1. 决策树的构造</a><ul><li><a href=#11--信息增益>1.1 信息增益</a></li><li><a href=#12--划分数据集>1.2 划分数据集</a></li><li><a href=#13--递归构建决策树>1.3 递归构建决策树</a></li></ul></li></ul></nav></div></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=copyright-author>奔赴下一场山海</span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i>
<span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script>
<script type=text/javascript src=/js/search.js></script>
<script type=text/javascript src=/js/affix.js></script>
<script type=text/javascript src=/js/scrollspy.js></script>
<script type=text/javascript>function detectIE(){var e=window.navigator.userAgent,t=e.indexOf("MSIE "),n=e.indexOf("Trident/"),s=e.indexOf("Edge/");return t>0||n>0||s>0?-1:1}function getCntViewHeight(){var t=$("#content").height(),e=$(window).height(),n=t>e?t-e:$(document).height()-e;return n}function getScrollbarWidth(){var e=$("<div />").addClass("scrollbar-measure").prependTo("body"),t=e[0],n=t.offsetWidth-t.clientWidth;return e.remove(),n}function registerBackTop(){var t=50,e=$(".back-to-top");$(window).on("scroll",function(){e.toggleClass("back-to-top-on",window.pageYOffset>t);var s=$(window).scrollTop(),o=getCntViewHeight(),i=s/o,n=Math.round(i*100),a=n>100?100:n;$("#scrollpercent>span").html(a)}),e.on("click",function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var e=".post-toc",s=$(e),t=".active-current";s.on("activate.bs.scrollspy",function(){var t=$(e+" .active").last();n(),t.addClass("active-current")}).on("clear.bs.scrollspy",n),$("body").scrollspy({target:e});function n(){$(e+" "+t).removeClass(t.substring(1))}}function initAffix(){var e=$(".header-inner").height(),t=parseInt($(".main").css("padding-bottom"),10),n=e+10;$(".sidebar-inner").affix({offset:{top:n,bottom:t}}),$(document).on("affixed.bs.affix",function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){$(window).on("resize",function(){e&&clearTimeout(e),e=setTimeout(function(){var e=document.body.clientHeight-100;updateTOCHeight(e)},0)}),updateTOCHeight(document.body.clientHeight-100);var e,t=getScrollbarWidth();$(".post-toc").css("width","calc(100% + "+t+"px)")}function updateTOCHeight(e){e=e||"auto",$(".post-toc").css("max-height",e)}$(function(){var e,t,n,s,o=$(".header-inner").height()+10;$("#sidebar").css({'margin-top':o}).show(),t=parseInt($("#sidebar").css("margin-top")),n=parseInt($(".sidebar-inner").css("height")),e=t+n,s=$(".content-wrap").height(),s<e&&$(".content-wrap").css("min-height",e),$(".site-nav-toggle").on("click",function(){var e=$(".site-nav"),o=$(".toggle"),t="site-nav-on",i="toggle-close",n=e.hasClass(t),a=n?"slideUp":"slideDown",s=n?"removeClass":"addClass";e.stop()[a]("normal",function(){e[s](t),o[s](i)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$(".sidebar-nav-toc").click(function(){$(this).addClass("sidebar-nav-active"),$(this).next().removeClass("sidebar-nav-active"),$("."+$(this).next().attr("data-target")).toggle(500),$("."+$(this).attr("data-target")).toggle(500)}),$(".sidebar-nav-overview").click(function(){$(this).addClass("sidebar-nav-active"),$(this).prev().removeClass("sidebar-nav-active"),$("."+$(this).prev().attr("data-target")).toggle(500),$("."+$(this).attr("data-target")).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script>
<script type=text/javascript>$(function(){$(".post-body").viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+"//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js",function(){new Waline({el:"#wcomments",visitor:!0,avatar:"wavatar",avatarCDN:"https://sdn.geekzu.org/avatar/",avatarForce:!1,wordLimit:"200",placeholder:" 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ",requiredFields:["nick","mail"],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$("#wcomments").html("抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。")})</script><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script>
<script>(function(){var t,e=document.createElement("script"),n=window.location.protocol.split(":")[0];n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>