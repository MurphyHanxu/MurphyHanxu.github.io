<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>DIDL3.2 Notes - 奔赴下一场山海</title><meta name=keywords content="博客,程序员,架构师,思考,读书,笔记,技术,分享,大数据,产品"><meta name=author content="Murphy"><meta property="og:title" content="DIDL3.2 Notes"><meta property="og:site_name" content="奔赴下一场山海"><meta property="og:image" content="/img/author.jpg"><meta name=title content="DIDL3.2 Notes - 奔赴下一场山海"><meta name=description content="欢迎来到Murphy的博客网站。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i>
<a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span>
<span class=site-title>奔赴下一场山海</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>但远山长，云山乱，晓山青。</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span>
<span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://murphyhanxu.github.io/post/dide3.2_notes/ itemprop=url>DIDL3.2 Notes</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i>
<span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2022-09-05">2022-09-05</time></span>
<span class=post-category>&nbsp; | &nbsp;
<i class="fa fa-folder-o fa-fw"></i>
<span class=post-meta-item-text>分类：</span>
<span itemprop=about itemscope itemtype=https://schema.org/Thing><a href=/categories/cs itemprop=url rel=index style=text-decoration:underline><span itemprop=name>CS</span></a>
&nbsp;</span></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i>
<span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>3321 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i>
<span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>7分钟</span></span>
<span id=/post/dide3.2_notes/ class=leancloud_visitors data-flag-title="DIDL3.2 Notes">|
<i class="fa fa-binoculars fa-fw"></i>
<span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><p>3.2</p><p>线性回归的从零开始实现</p><h2 id=1生成数据集>1.生成数据集</h2><p>为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。我们将使用低维数据，这样可以很容易地将其可视化。在下面的代码中，我们生成一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的2个特征。我们合成的数据集是一个矩阵$\mathbf{X}\in\mathbb{R}^{1000\times2}$。</p><p>我们使用线性模型参数$\mathbf{w}=[2,-3.4]^{T}$、$b=4.2$和噪声项$\epsilon$生成数据集及其标签：
$$
\mathbf{y}=\mathbf{Xw}+b+\epsilon \tag{3.2.1}
$$</p><p>你可以将$\epsilon$视为模型预测和标签时的潜在观测误差。在这里我们认为标准假设成立，即$\epsilon$服从均值为$0$的正太分布。为了简化问题，我们将标准差设为$0.01$。下面的代码生成合成数据集。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>synthetic_data</span>(w, b, num_examples):  <span style=color:#080;font-style:italic>#@save</span>
</span></span><span style=display:flex><span>    <span style=color:#b44>&#34;&#34;&#34;生成y=Xw+b+噪声&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    X <span style=color:#666>=</span> torch<span style=color:#666>.</span>normal(<span style=color:#666>0</span>, <span style=color:#666>1</span>, (num_examples, <span style=color:#a2f>len</span>(w))) <span style=color:#080;font-style:italic># X的形状是num_examples x len(w)</span>
</span></span><span style=display:flex><span>    y <span style=color:#666>=</span> torch<span style=color:#666>.</span>matmul(X, w) <span style=color:#666>+</span> b <span style=color:#080;font-style:italic># y的形状是1 x num_examples</span>
</span></span><span style=display:flex><span>    y <span style=color:#666>+=</span> torch<span style=color:#666>.</span>normal(<span style=color:#666>0</span>, <span style=color:#666>0.01</span>, y<span style=color:#666>.</span>shape) <span style=color:#080;font-style:italic># 将y加上epsilon</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> X, y<span style=color:#666>.</span>reshape((<span style=color:#666>-</span><span style=color:#666>1</span>, <span style=color:#666>1</span>)) <span style=color:#080;font-style:italic># 返回噪声集x,y</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># torch.normal(mean, std, (number of samples, dimension))</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># torch.matmul(A, B)指矩阵乘法</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># .reshape((-1,1))转换成一列</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>true_w <span style=color:#666>=</span> torch<span style=color:#666>.</span>tensor([<span style=color:#666>2</span>, <span style=color:#666>-</span><span style=color:#666>3.4</span>]) <span style=color:#080;font-style:italic># w的形状为1 x 2</span>
</span></span><span style=display:flex><span>true_b <span style=color:#666>=</span> <span style=color:#666>4.2</span>
</span></span><span style=display:flex><span>features, labels <span style=color:#666>=</span> synthetic_data(true_w, true_b, <span style=color:#666>1000</span>)
</span></span></code></pre></div><p>则features中的每一行都包含一个二维数据样本，labels中的每一行都包含一维标签值（一个标量）。</p><p>通过生成第二个特征features[:,1]和labels的散点图，可以直观观察到两者之间的线性关系。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>d2l<span style=color:#666>.</span>set_figsize()
</span></span><span style=display:flex><span>d2l<span style=color:#666>.</span>plt<span style=color:#666>.</span>scatter(features[:, (<span style=color:#666>1</span>)]<span style=color:#666>.</span>detach()<span style=color:#666>.</span>numpy(), labels<span style=color:#666>.</span>detach()<span style=color:#666>.</span>numpy(), <span style=color:#666>1</span>) 
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 1表示的是样本点在图像上的大小</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># .detach() 返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。</span>
</span></span><span style=display:flex><span>d2l<span style=color:#666>.</span>plt<span style=color:#666>.</span>show()
</span></span></code></pre></div><p><img src=https://MurphyHanxu.github.io/blogs-images/images/DIDL3.2.1.png alt=3.2.1></p><h2 id=2读取数据集>2.读取数据集</h2><p>回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数，该函数能打乱数据集中的样本并以小批量方式获取数据。</p><p>在下面的代码中，我们定义一个data_iter函数，该函数接收批量大小、特征矩阵和标签向量作为插入，生成大小为batch_size的小批量。每个小批量包含一组特征和标签。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>data_iter</span>(batch_size, features, labels):
</span></span><span style=display:flex><span>    num_examples <span style=color:#666>=</span> <span style=color:#a2f>len</span>(features)
</span></span><span style=display:flex><span>    indices <span style=color:#666>=</span> <span style=color:#a2f>list</span>(<span style=color:#a2f>range</span>(num_examples))
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic># 这些样本是随机读取的，没有特定的顺序</span>
</span></span><span style=display:flex><span>    random<span style=color:#666>.</span>shuffle(indices)
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic># 将列表所有的元素随机排列</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> i <span style=color:#a2f;font-weight:700>in</span> <span style=color:#a2f>range</span>(<span style=color:#666>0</span>, num_examples, batch_size):
</span></span><span style=display:flex><span>        batch_indices <span style=color:#666>=</span> torch<span style=color:#666>.</span>tensor(
</span></span><span style=display:flex><span>            indices[i: <span style=color:#a2f>min</span>(i <span style=color:#666>+</span> batch_size, num_examples)])
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>yield</span> features[batch_indices], labels[batch_indices]
</span></span><span style=display:flex><span>        <span style=color:#080;font-style:italic># 迭代器生成随机顺序的features和labels</span>
</span></span></code></pre></div><p>通常，我们利用GPU并行运算的优势，处理合理大小的“小批量”。每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行计算。GPU可以在处理几百个样本时，所花费的时间不比处理一个样本时间多太多。</p><p>我们直观感受一下小批量运算：读取第一个小批量数据样本并打印。每个批量的特征维度显示批量大小和输入特征数。同样的，批量的标签形状与batch_size相等。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>batch_size <span style=color:#666>=</span> <span style=color:#666>10</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> X, y <span style=color:#a2f;font-weight:700>in</span> data_iter(batch_size, features, labels):
</span></span><span style=display:flex><span>    <span style=color:#a2f>print</span>(X, <span style=color:#b44>&#39;</span><span style=color:#b62;font-weight:700>\n</span><span style=color:#b44>&#39;</span>, y)
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>break</span>
</span></span></code></pre></div><p>当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。上面实现的迭代对于教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。在深度学习框架中实现的内置迭代器效率要高得多，它可以处理存储在文件中的数据和数据流提供的数据。</p><h2 id=3初始化模型参数>3.初始化模型参数</h2><p>在我们开始用小批量随机梯度下降优化我们的模型参数之前，我们需要先有一些参数。在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>w <span style=color:#666>=</span> torch<span style=color:#666>.</span>normal(<span style=color:#666>0</span>, <span style=color:#666>0.01</span>, size<span style=color:#666>=</span>(<span style=color:#666>2</span>,<span style=color:#666>1</span>), requires_grad<span style=color:#666>=</span><span style=color:#a2f;font-weight:700>True</span>)
</span></span><span style=display:flex><span>b <span style=color:#666>=</span> torch<span style=color:#666>.</span>zeros(<span style=color:#666>1</span>, requires_grad<span style=color:#666>=</span><span style=color:#a2f;font-weight:700>True</span>)
</span></span></code></pre></div><p>在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。每次更新都需要计算损失函数关于模型参数的梯度。有了这个梯度，我们就可以向减小损失的方向更新每个参数。因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。我们使用2.5节中引入的自动微分来计算梯度。</p><h2 id=4定义模型>4.定义模型</h2><p>接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。回想一下，要计算线性模型的输出，我们只需计算输入特征$\mathbf{X}$和模型权重$\mathbf{w}$的矩阵-向量乘法后向上偏置$b$。注意，上面的$\mathbf{Xw}$是一个向量，而$b$是一个标量。回想广播机制：当我们用一个向量加一个标量时，标量会被加到向量的每个分量上。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>linreg</span>(X, w, b):  <span style=color:#080;font-style:italic>#@save</span>
</span></span><span style=display:flex><span>    <span style=color:#b44>&#34;&#34;&#34;线性回归模型&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> torch<span style=color:#666>.</span>matmul(X, w) <span style=color:#666>+</span> b
</span></span></code></pre></div><h2 id=5定义损失函数>5.定义损失函数</h2><p>因为需要计算损失函数的梯度，所以我们应该先定义损失函数。这里我们使用平方损失函数。在实现中，我们需要将真实值y的形状转换为和预测值y_hat的形状相同。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>squared_loss</span>(y_hat, y):  <span style=color:#080;font-style:italic>#@save</span>
</span></span><span style=display:flex><span>    <span style=color:#b44>&#34;&#34;&#34;均方损失&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> (y_hat <span style=color:#666>-</span> y<span style=color:#666>.</span>reshape(y_hat<span style=color:#666>.</span>shape)) <span style=color:#666>**</span> <span style=color:#666>2</span> <span style=color:#666>/</span> <span style=color:#666>2</span>
</span></span></code></pre></div><h2 id=6定义优化算法>6.定义优化算法</h2><p>尽管线性回归有解析解，但本书中的其他模型却没有。这里我们介绍小批量随机梯度下降。</p><p>在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。接下来，朝着减少损失的方向更新我们的参数。下面的函数实现小批量随机梯度下降更新。该函数接受模型参数集合、学习速率和批量大小作为输入。每一步更新的大小由学习速率lr决定。因为我们计算的损失是一个批量样本的总和，所以我们用批量大小(batch_size)来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>sgd</span>(params, lr, batch_size):  <span style=color:#080;font-style:italic>#@save</span>
</span></span><span style=display:flex><span>    <span style=color:#b44>&#34;&#34;&#34;小批量随机梯度下降&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>with</span> torch<span style=color:#666>.</span>no_grad():
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic># 更新的时候不参与梯度计算</span>
</span></span><span style=display:flex><span>        <span style=color:#a2f;font-weight:700>for</span> param <span style=color:#a2f;font-weight:700>in</span> params:
</span></span><span style=display:flex><span>            param <span style=color:#666>-=</span> lr <span style=color:#666>*</span> param<span style=color:#666>.</span>grad <span style=color:#666>/</span> batch_size
</span></span><span style=display:flex><span>            param<span style=color:#666>.</span>grad<span style=color:#666>.</span>zero_()
</span></span><span style=display:flex><span>            <span style=color:#080;font-style:italic># 将梯度重新归零，以便下一次计算梯度</span>
</span></span></code></pre></div><h2 id=7训练>7.训练</h2><p>现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。理解这段代码至关重要，因为从事深度学习后，你会一遍又一遍地看到几乎相同的训练过程。在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。计算完损失后，我们开始反向传播，存储每个参数的梯度。最后，我们调用优化算法sgd来更新模型参数。</p><p>概括一下，我们将执行一下循环：</p><ul><li>初始化参数</li><li>重复一下训练，直到完成<ul><li>计算梯度$\mathbf{g}\leftarrow\partial_{(\mathbf{w},b)}\frac{1}{\vert\mathbf{B}\vert}\sum_{i\in\mathbf{B}}l(X^{(i)},y{(i)},\mathbf{w},b)$</li><li>更新参数$(\mathbf{w},b)\leftarrow(\mathbf{w},b)-\eta\mathbf{g}$</li></ul></li></ul><p>在每个迭代周期(epoch)中，我们使用data_iter函数遍历整个数据集，并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数num_epochs和学习率lr都是超参数，分别设为$3$和$0.03$。设置超参数很棘手，需要通过反复试验进行调整。我们现在忽略这些细节，以后会在11节中详细介绍。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lr <span style=color:#666>=</span> <span style=color:#666>0.03</span> <span style=color:#080;font-style:italic># 学习率</span>
</span></span><span style=display:flex><span>num_epochs <span style=color:#666>=</span> <span style=color:#666>3</span> <span style=color:#080;font-style:italic># 迭代次数（周期）</span>
</span></span><span style=display:flex><span>net <span style=color:#666>=</span> linreg
</span></span><span style=display:flex><span>loss <span style=color:#666>=</span> squared_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> epoch <span style=color:#a2f;font-weight:700>in</span> <span style=color:#a2f>range</span>(num_epochs):
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> X, y <span style=color:#a2f;font-weight:700>in</span> data_iter(batch_size, features, labels):
</span></span><span style=display:flex><span>        l <span style=color:#666>=</span> loss(net(X, w, b), y)  <span style=color:#080;font-style:italic># X和y的小批量损失</span>
</span></span><span style=display:flex><span>        <span style=color:#080;font-style:italic># 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，</span>
</span></span><span style=display:flex><span>        <span style=color:#080;font-style:italic># 并以此计算关于[w,b]的梯度</span>
</span></span><span style=display:flex><span>        l<span style=color:#666>.</span>sum()<span style=color:#666>.</span>backward()
</span></span><span style=display:flex><span>        sgd([w, b], lr, batch_size)  <span style=color:#080;font-style:italic># 使用参数的梯度更新参数</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>with</span> torch<span style=color:#666>.</span>no_grad():
</span></span><span style=display:flex><span>        train_l <span style=color:#666>=</span> loss(net(features, w, b), labels)
</span></span><span style=display:flex><span>        <span style=color:#a2f>print</span>(<span style=color:#b44>f</span><span style=color:#b44>&#39;epoch </span><span style=color:#b68;font-weight:700>{</span>epoch <span style=color:#666>+</span> <span style=color:#666>1</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>, loss </span><span style=color:#b68;font-weight:700>{</span><span style=color:#a2f>float</span>(train_l<span style=color:#666>.</span>mean())<span style=color:#b68;font-weight:700>:</span><span style=color:#b44>f</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#39;</span>)
</span></span></code></pre></div><p>因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。因此，我们可以通过比较真实参数和通过训练学到的参数来评估训练的成功程度。事实上，真实参数和通过训练学到的参数确实非常接近。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a2f>print</span>(<span style=color:#b44>f</span><span style=color:#b44>&#39;w的估计误差: </span><span style=color:#b68;font-weight:700>{</span>true_w <span style=color:#666>-</span> w<span style=color:#666>.</span>reshape(true_w<span style=color:#666>.</span>shape)<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#a2f>print</span>(<span style=color:#b44>f</span><span style=color:#b44>&#39;b的估计误差: </span><span style=color:#b68;font-weight:700>{</span>true_b <span style=color:#666>-</span> b<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#39;</span>)
</span></span></code></pre></div><p><img src=https://MurphyHanxu.github.io/blogs-images/images/DIDL3.2.2.png alt=3.2.1></p><p>注意，我们不应该想当然地认为我们能够完美地求解参数。在机器学习中，我么通常不太关心恢复真正的参数，而更关心如何高度准确预测参数。幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。其中一个原因是，在深度学习网络中存在许多参数组合能够实现高度精确的预测。</p></div><footer class=post-footer><div class=post-tags><a href=/tags/didl rel=tag title=DIDL>#DIDL#</a></div><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.jpg width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>DIDL3.2 Notes</p><p><span>链接：</span>https://murphyhanxu.github.io/post/dide3.2_notes/</p><p><span>作者：</span>Murphy</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick='var qr=document.getElementById("QR");qr.style.display==="none"?qr.style.display="block":qr.style.display="none"'>
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://murphyhanxu.github.io/post/note-of-numerical-experiments/ rel=next title="Notes of Numerical Experiments"><i class="fa fa-chevron-left"></i> Notes of Numerical Experiments</a></div><div class="post-nav-prev post-nav-item"><a href=https://murphyhanxu.github.io/post/k-nearest-neighbor-algorithm2/ rel=prev title=k近邻算法--2.示例：使用k近邻算法改进约会网站的配对效果>k近邻算法--2.示例：使用k近邻算法改进约会网站的配对效果
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
<span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
<span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>文章目录</li><li class=sidebar-nav-overview data-target=site-overview>站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/avatar.png alt=Murphy><p class=site-author-name itemprop=name>Murphy</p><p class="site-description motion-element" itemprop=description>谁的脸谁的姓名</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>18</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>1</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>8</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/MurphyHanxu target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>
GitHub</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li><li class=links-of-blogroll-item><a href=https://math.mickeylili.com/ title=Mickeylili target=_blank>Mickeylili</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>
标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/machine-learning>Machine learning</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/github-pages>Github pages</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python>Python</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/hugo>Hugo</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/sql>SQL</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/didl>Didl</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/math>Math</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/mathematical-modeling>Mathematical modeling</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1生成数据集>1.生成数据集</a></li><li><a href=#2读取数据集>2.读取数据集</a></li><li><a href=#3初始化模型参数>3.初始化模型参数</a></li><li><a href=#4定义模型>4.定义模型</a></li><li><a href=#5定义损失函数>5.定义损失函数</a></li><li><a href=#6定义优化算法>6.定义优化算法</a></li><li><a href=#7训练>7.训练</a></li></ul></nav></div></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=copyright-author>奔赴下一场山海</span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i>
<span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script>
<script type=text/javascript src=/js/search.js></script>
<script type=text/javascript src=/js/affix.js></script>
<script type=text/javascript src=/js/scrollspy.js></script>
<script type=text/javascript>function detectIE(){var e=window.navigator.userAgent,t=e.indexOf("MSIE "),n=e.indexOf("Trident/"),s=e.indexOf("Edge/");return t>0||n>0||s>0?-1:1}function getCntViewHeight(){var t=$("#content").height(),e=$(window).height(),n=t>e?t-e:$(document).height()-e;return n}function getScrollbarWidth(){var e=$("<div />").addClass("scrollbar-measure").prependTo("body"),t=e[0],n=t.offsetWidth-t.clientWidth;return e.remove(),n}function registerBackTop(){var t=50,e=$(".back-to-top");$(window).on("scroll",function(){e.toggleClass("back-to-top-on",window.pageYOffset>t);var s=$(window).scrollTop(),o=getCntViewHeight(),i=s/o,n=Math.round(i*100),a=n>100?100:n;$("#scrollpercent>span").html(a)}),e.on("click",function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var e=".post-toc",s=$(e),t=".active-current";s.on("activate.bs.scrollspy",function(){var t=$(e+" .active").last();n(),t.addClass("active-current")}).on("clear.bs.scrollspy",n),$("body").scrollspy({target:e});function n(){$(e+" "+t).removeClass(t.substring(1))}}function initAffix(){var e=$(".header-inner").height(),t=parseInt($(".main").css("padding-bottom"),10),n=e+10;$(".sidebar-inner").affix({offset:{top:n,bottom:t}}),$(document).on("affixed.bs.affix",function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){$(window).on("resize",function(){e&&clearTimeout(e),e=setTimeout(function(){var e=document.body.clientHeight-100;updateTOCHeight(e)},0)}),updateTOCHeight(document.body.clientHeight-100);var e,t=getScrollbarWidth();$(".post-toc").css("width","calc(100% + "+t+"px)")}function updateTOCHeight(e){e=e||"auto",$(".post-toc").css("max-height",e)}$(function(){var e,t,n,s,o=$(".header-inner").height()+10;$("#sidebar").css({'margin-top':o}).show(),t=parseInt($("#sidebar").css("margin-top")),n=parseInt($(".sidebar-inner").css("height")),e=t+n,s=$(".content-wrap").height(),s<e&&$(".content-wrap").css("min-height",e),$(".site-nav-toggle").on("click",function(){var e=$(".site-nav"),o=$(".toggle"),t="site-nav-on",i="toggle-close",n=e.hasClass(t),a=n?"slideUp":"slideDown",s=n?"removeClass":"addClass";e.stop()[a]("normal",function(){e[s](t),o[s](i)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$(".sidebar-nav-toc").click(function(){$(this).addClass("sidebar-nav-active"),$(this).next().removeClass("sidebar-nav-active"),$("."+$(this).next().attr("data-target")).toggle(500),$("."+$(this).attr("data-target")).toggle(500)}),$(".sidebar-nav-overview").click(function(){$(this).addClass("sidebar-nav-active"),$(this).prev().removeClass("sidebar-nav-active"),$("."+$(this).prev().attr("data-target")).toggle(500),$("."+$(this).attr("data-target")).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script>
<script type=text/javascript>$(function(){$(".post-body").viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+"//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js",function(){new Waline({el:"#wcomments",visitor:!0,avatar:"wavatar",avatarCDN:"https://sdn.geekzu.org/avatar/",avatarForce:!1,wordLimit:"200",placeholder:" 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ",requiredFields:["nick","mail"],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$("#wcomments").html("抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。")})</script><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script>
<script>(function(){var t,e=document.createElement("script"),n=window.location.protocol.split(":")[0];n==="https"?e.src="https://zz.bdstatic.com/linksubmit/push.js":e.src="http://push.zhanzhang.baidu.com/push.js",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>